```
å¼‚æ­¥æ•°æ®å¹¶è¡Œæ¯æ­¥è¿­ä»£éœ€è¦å…ˆä»å‚æ•°æœåŠ¡å™¨è·å–å‚æ•°ï¼Œè®¡ç®—å®Œ
æ¢¯åº¦åè¿˜è¦å°†æ¢¯åº¦å€¼ä¼ å›å‚æ•°æœåŠ¡å™¨ã€‚ç›¸æ¯”å•æœºè®­ç»ƒï¼Œå¼‚æ­¥æ•°æ®å¹¶è¡Œéœ€è¦é¢å¤–çš„é€šä¿¡å¼€
é”€ï¼Œå› è€Œæ— æ³•è¾¾åˆ°çº¿æ€§åŠ é€Ÿæ¯”ã€‚å‡è®¾å·¥ä½œè€…æ¯æ­¥è¿­ä»£å¹³å‡è€—æ—¶ä¸ºğ‘‡ï¿½
ï¿½
ï¼Œå¹³å‡è®¡ç®—å¼€é”€ä¸ºğ‘‡ï¿½

å¹³å‡é€šä¿¡è€—æ—¶ä¸ºğ‘‡

ï¼Œè®¡ç®—å¼€é”€å æ¯”å®šä¹‰ä¸ºï¼š 

é€šä¿¡å¼€é”€å æ¯”å®šä¹‰ä¸ºï¼š 

```

### RDD

```
rddçš„ç¼ºç‚¹æ˜¯ï¼Œæ— æ³•æ§åˆ¶åˆ†å¸ƒçš„ç»†èŠ‚ã€‚å¦‚ï¼Œæ— æ³•æŒ‡å®šç‰¹å®šæœºå™¨è·å–ç‰¹å®šçš„æ•°æ®ï¼Œè¿™äº›éƒ½æœ‰sparkè¿›è¡Œäº†é«˜åº¦å°è£…ï¼Œæ²¡æœ‰æš´éœ²ç»™ç”¨æˆ·ï¼Œç”¨æˆ·åªèƒ½åƒç¼–å†™å•æœºç¨‹åºä¸€æ ·ç¼–å†™ç¨‹åºã€‚
```



##### åˆ›å»º

```
rddMine=sc.parallelize(listï¼Œ3)
listæ˜¯åˆ—è¡¨ï¼Œå¯ä»¥å­˜å‚¨ä»»æ„ç±»åˆ«çš„å¤šä¸ªå…ƒç´ ã€‚
3æ˜¯åˆ†åŒºæ•°ï¼Œlistçš„æ•°æ®è¢«åˆ†æ•£åœ¨3ä¸ªåˆ†åŒºä¸Šã€‚listä¸­å…±ç”¨len(list)ä¸ªå…ƒç´ ï¼Œå•ä¸ªå…ƒç´ æ˜¯åˆ’åˆ†åˆ°åˆ†åŒºçš„æœ€å°çš„å•ä½ã€‚
rddMine.mapPartion(train).collect
mapPartionä¸ºæ¯ä¸ªpartionä¸Šçš„æ•°æ®åˆ†åˆ«è°ƒç”¨trainå‡½æ•°ï¼Œtrainå‡½æ•°å®šä¹‰æ—¶åªèƒ½æœ‰ä¸€ä¸ªå‚æ•°ï¼Œå³rddã€‚
```

### åˆ›å»ºsparkcontext

```
SparkSession.builder.getOrCreate().sparkContext
```

ä»liståˆ›å»ºrdd

```
pairs = [(x, y) for x, y in zip(features, labels)]
sc.parallelize(pairs)
```

### problem

```
Exception: Python in worker has different version 3.9 than that in driver 3.7, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
worker,driverçš„pythonç‰ˆæœ¬ä¸åŒ¹é…ï¼Œä½†æ˜¯ä¸¤è€…ä¸éƒ½æ˜¯æœ¬æœºå˜›ã€‚
```

### æ‰§è¡Œæµç¨‹

>https://blog.csdn.net/u010745505/article/details/81261019

```
æ³¨æ„ï¼šactionæ“ä½œæ‰ä¼šè§¦å‘sparkContextçš„runjobæ–¹æ³•ã€‚ç„¶åè¿›è¡Œstageå’Œtaskçš„åˆ’ï¼Œzauè¿›è¡Œèµ„æºåˆ†é…å’Œä»»åŠ¡è°ƒåº¦ã€‚è¿™ä¸ªrddä¸Šçš„è¿‡ç¨‹æ˜¯ä¸å¯æ§çš„...ï¼Œè€Œmapreduceæ˜¯å¯ä»¥é€šè¿‡mapreduceè¿›è¡Œæ§åˆ¶çš„ï¼Œä½†mapreduceå±€é™äº†è¡¨è¾¾æ–¹å¼ã€‚å°±ç®—ç”¨rddï¼Œrddä¹Ÿåªæ˜¯ç”¨æ¥ä¼ é€’æ•°æ®å’Œé€šä¿¡å§ã€‚å®é™…ä¸Šmapreduceå’Œrddæ§åˆ¶çš„æ˜¯
1ã€Driverç¨‹åºçš„ä»£ç è¿è¡Œåˆ°actionæ“ä½œï¼Œè§¦å‘äº†SparkContextçš„runJobæ–¹æ³•ã€‚ 
2ã€SparkContextè°ƒç”¨DAGSchedulerçš„runJobå‡½æ•°ã€‚ 
3ã€DAGScheduleræŠŠJobåˆ’åˆ†stageï¼Œç„¶åæŠŠstageè½¬åŒ–ä¸ºç›¸åº”çš„Tasksï¼ŒæŠŠTasksäº¤ç»™TaskSchedulerã€‚ 
4ã€é€šè¿‡TaskScheduleræŠŠTasksæ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—å½“ä¸­ï¼Œäº¤ç»™SchedulerBackendè¿›è¡Œèµ„æºåˆ†é…å’Œä»»åŠ¡è°ƒåº¦ã€‚ 
5ã€è°ƒåº¦å™¨ç»™Taskåˆ†é…æ‰§è¡ŒExecutorï¼ŒExecutorBackendè´Ÿè´£æ‰§è¡ŒTaskã€‚

```



### å…³äºå®‰è£…

```
ä»å®˜ç½‘ä¸‹è½½sparkçš„å‹ç¼©åŒ…ï¼Œå…¶ä¸åŒºåˆ†å¹³å°æ— éœ€ç¼–è¯‘ï¼Œè¯´æ˜æ˜¯ç”±javaè¯­è¨€å’Œscalaç¼–å†™çš„æºç æˆ–å­—èŠ‚ç æˆ–jaråŒ…ï¼Œç„¶åç”±javaè™šæ‹Ÿæœºè¿›è¡Œè·¨å¹³å°ç¼–è¯‘æ‰§è¡Œã€‚
è¿›å…¥binç›®å½•ï¼Œpysparkå³å¯å¯åŠ¨äº¤äº’å‘½ä»¤è¡Œå’Œwebuiï¼Œå…¶æ˜¯ç”±javaç¼–å†™çš„ç¨‹åºï¼Œå¸¦æœ‰webç¨‹åºåç«¯å’Œhtmlã€‚
```

### sparkæäº¤è¿œç¨‹jaråŒ…

```
pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5
æ­¤æ“ä½œä¼šå»repo1.maven.orgä¸‹è½½å¯¹åº”jaråŒ…å’Œä¾èµ–ã€‚
```

### pysaprk

```
#æœ¬åœ°ç¼–è¯‘å™¨è°ƒè¯•è¿è¡Œï¼Œä»¥setHadoophomeå½¢å¼æ§åˆ¶æ•´ä¸ªé€»è¾‘ã€‚
sparkå’Œhadoopéƒ½ä»¥hdfs//æˆ–spark//æ¥å£å½¢å¼æ¥å—è¯·æ±‚å’Œæ§åˆ¶ä½œä¸šã€‚
è¿™ç§æƒ…å†µhadoopæˆ–sparkæä¾›äº†æ‰§è¡Œç¯å¢ƒï¼Œç»“æœå†™å‡ºåˆ°æœ¬åœ°ã€‚
pythonè´Ÿè´£ç¼–å†™é€»è¾‘ï¼Œæäº¤åˆ°æœåŠ¡ç«¯sparkåï¼Œsparkè¿›è¡Œ
```

##### æäº¤

>å…³äºæäº¤å‚æ•°:
>
>https://books.japila.pl/apache-spark-internals/tools/spark-submit/#command-line-options_1
>
>https://blog.csdn.net/weixin_42649077/article/details/84976960

```
spark-submit
--master  spark://hbase:7077
--py-files /home/uther/uther/uther.zip
--conf "spark.pyspark.driver.python=""
--conf  "spark.pyspark.python=""
--files 1.zip
--py-files  2.py,3.py
main.py

æ³¨æ„æ‰“åŒ…æ—¶è¿›å…¥è¦æ‰“åŒ…çš„æ–‡ä»¶å¤¹å†…ï¼Œç„¶åï¼Œzip -r test.zip ./*ã€‚å†å¤–éƒ¨æ‰“åŒ…ä¼šå¤šä¸€å±‚æ–‡ä»¶å¤¹ï¼Œå¯¼è‡´æ— æ³•è®¿é—®ã€‚
è®¿é—®è·¯å¾„
--py-filesç”¨äºæäº¤å¤šä¸ªå•ä¸€çš„pyæ–‡ä»¶
--filesç”¨äºæäº¤æ‰“åŒ…çš„zip
è®¿é—®è·¯å¾„ç±»ä¼¼å¦‚ä¸‹ï¼š
å¯¹äºzipåŒ…å†…çš„testæ–‡ä»¶å¤¹ä¸‹çš„1.jsonæ–‡ä»¶,åœ¨pythonç¨‹åºä¸­è¿™æ ·è®¿é—®
open('test/1.json','r') as f

å¯¹äºæäº¤çš„zipä¸­çš„pyæ–‡ä»¶ï¼Œè¦ä½¿ç”¨å¦‚ä¸‹æ–¹å¼è®¿é—®
å¯¹äºtest.zipæ ¹ä¸‹çš„util.pyï¼š
from util  import start
```

##### spark-submit .sh

```
spark-submit 
--master spark://172.18.65.187:7077
--py-files /root/software/spark-elephas/spark_elephas.zip 
--conf "spark.pyhspark.driver.python=/root/miniconda3/envs/elephas1/bin/python" 
--conf "spark.pyspark.python=/root/miniconda3/envs/elephas1/bin/python"  
/root/software/spark-elephas/myModel.py
```



### sparkå®‰è£…

>spark-3.1.2-bin-hadoop3.2,é»˜è®¤weç«¯å£8080,ç»‘å®šå¤±è´¥ä¼šç”¨8081ç«¯å£ã€‚

````
å®‰è£…æ•™ç¨‹:http://spark.apache.org/docs/latest/
````

##### æœ¬åœ°æ¨¡å¼

```
./bin/run-example SparkPi 10
./bin/spark-shell --master local[2]
```

##### standaloneæ¨¡å¼

```
å¤åˆ¶å‹ç¼©åŒ…è§£å‹å³å¯
sbin/start-master.sh
netstat -lntp
curl localhost:8081
```

##### ä¸ä½¿ç”¨yarnç­‰é›†ç¾¤ï¼Œå¼€å¯é›†ç¾¤

```
sbin/start-master.sh   
curl  http://hbase:8081
sbin/start-worker.sh  spark://hbase:7077
```



##### yarnæ¨¡å¼é›†ç¾¤

```
å®˜ç½‘:http://spark.apache.org/docs/latest/running-on-yarn.html
3.1.2_version:https://spark.apache.org/docs/3.1.2/spark-standalone.html
```

```
vim spark-en.sh
export HADOOP_CONF_DIR=..../
export YARN_CONF_DIR=..../
./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    examples/jars/spark-examples*.jar \
    10
```



### scalaå®‰è£…

```
ä¸‹è½½scalaåŒ…æˆ–åœ¨ideaçš„project structureä¸­æŒ‡å®šè‡ªåŠ¨ä¸‹è½½ï¼Œåœ¨settingçš„pluginsä¸­ä¸‹è½½scalaæ’ä»¶ã€‚
åˆ›å»ºmavené¡¹ç›®ï¼Œåœ¨project structureä¸­æ·»åŠ scalaã€‚
sprk3.0.0,scala-2.12
```



##### RDD

```
RDDä¸­æ“ä½œåˆ†ä¸ºTransformation,Actionã€‚å¯¹äºTransformationæ˜¯ç´¯ç§¯çš„ï¼Œå½“æœ‰Actionæ‰§è¡Œæ—¶æ‰ä¼šæ‰§è¡Œç´¯ç§¯çš„Transformationã€‚Actionå¸¸è§çš„æœ‰collectï¼Œmapï¼Œreduceï¼Œgroupbyç­‰ã€‚
mapæ˜¯å¯¹ä¸€ä¸ªæ•°ç»„çš„å•ä¸ªå¯¹è±¡é‡å¤è°ƒç”¨mapå‡½æ•°å†…å®¹ã€‚
reduceæ˜¯å°†å¤šä¸ªå¯¹è±¡é€’å½’è°ƒç”¨ï¼Œæœ€ç»ˆå½’çº¦ä¸ºä¸€ä¸ªå€¼ã€‚
```

