[TOC]
# Imooc
>使用Spark SQL分析Imooc访问日志,数据以日志文件形式提供，共一千万条访问日志，5G数据量。
>主要完成如下指标统计：
>1.某天最后欢迎的TopN课程
>2.某天各个省市各自的TopN课程
>3.按照流量统计TopN课程
>4.某天最受欢迎的文章
>5.某天进行code最多的课程
>6.某天最勤奋的IP

>在使用spark开发时，可以以shell脚本形式编写sql进行数据处理，也可以使用scala借助rdd，table sql等处理数据，对于实时数仓也要使用scala操作DStream进行数据处理。

## 日志

>日志不是json格式的，是日志插件打印的原生格式。一般如果使用了自定义的埋点采集数据，都会为了规范定义统一的json格式，如果没有定义，那么就只能拿日志插件输出的内容了。
```
60.165.39.1 - - [10/Nov/2016:00:01:53 +0800] "POST /course/ajaxmediauser HTTP/1.1" 200 54 "www.imooc.com" "http://www.imooc.com/code/1431" mid=1431&time=60 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36 SE 2.X MetaSr 1.0" "-" 10.100.136.64:80 200 0.014 0.014
14.145.74.175 - - [10/Nov/2016:00:01:53 +0800] "POST /course/ajaxmediauser/ HTTP/1.1" 200 54 "www.imooc.com" "http://www.imooc.com/video/678" mid=678&time=60&learn_time=551.5 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36" "-" 10.100.136.64:80 200 0.014 0.014.145.74.175 - - [10/Nov/2016:00:01:53 +0800] "POST /course/ajaxmediauser/ HTTP/1.1" 200 54 "www.imooc.com" "http://www.imooc.com/video/678" mid=678&time=60&learn_time=551.5 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36" "-" 10.100.136.64:80 200 0.014 0.014 
```

## 统计流程
### 日志清洗
>url,traffic,ip等多个值之间是用"空格"分隔开的，以此为依据取出对应的值。其中time属性，需要编写format解析格式，并转换为规范的格式。
>然后过滤局域网的的ip，和不符合要求的记录。因为后边要根据公网ip映射对应的地区城市，所以进行了过滤。
>最终集中到一个分区上，输出到同一个文件中。
```
package com.imooc

import com.imooc.conf.pathConfig
import com.imooc.utils.{DateUtils, FileUtils}
import org.apache.spark.{SparkConf, SparkContext}

object CleanRawLog {
  def main(args: Array[String]): Unit = {
    println("CleanRawLog")
    FileUtils.deleteDir(pathConfig.rawCleanOutputPath)
    val sparkConf:SparkConf=new SparkConf().setAppName("CleanRawLog").setMaster("local[*]")
    val sc=new SparkContext(sparkConf)
    println(sc)
//    每行为rdd的一个元素
    val rawLog=sc.textFile(pathConfig.protocol+pathConfig.rawPath)
//    rawLog.take(10).foreach(println)

//    想调试split的结果,观察split列表对应的值
    val splitsList=rawLog.map(line=>{
      val splits=line.split(" ")
      splits
    })
//    val temp=splitsList.take(10)
//调试完了，取所需的值。
// 值在split列表中对应的index位置
//0 ip
//3 4 time
//9 traffic
//12 url
    val parseResultList=splitsList.map(splits=>{
      val ip=splits(0)
      val time=splits(3)+splits(4)
      val traffic=splits(9)
      val url=splits(12).replaceAll("\"","")
//      时间需要格式化，url中去掉"
      List(DateUtils.parse(time),url,traffic,ip)
    })
//    过滤
    parseResultList.filter(result => !"10.100.0.1".equals(result(3)))
      .filter(result => !"-".equals(result(1)))
      .map(result => result(0)+"\t"+result(1)+"\t"+result(2)+"\t"+result(3))
      .repartition(1)
      .saveAsTextFile(pathConfig.protocol+pathConfig.rawCleanOutputPath)
  }
}

```

>为了得到更丰富的信息，对上一步的结果进行解析，获得更多的属性。
>包括对url内容进行解析，获得cmsType，cmsId，根据ip映射城市，根据时间解析日期等

```
package com.imooc

import com.ggstar.util.ip.IpHelper
import com.imooc.conf.pathConfig
import com.imooc.utils.{DateUtils, RegexUtils}
import org.apache.commons.lang3.StringUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}

object ExtractMore {
  case class Info(url:String,urlType:String, urlId:Long, traffic:String, ip:String, city:String, time:String, date:String)
  def main(args: Array[String]): Unit = {
//    读取
    val sparkConf:SparkConf=new SparkConf().setMaster("local[*]").setAppName("ExtractMore")
    val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    val sc:SparkContext=spark.sparkContext
    val input=sc.textFile(pathConfig.protocol+pathConfig.ExtractInputPath)
    val splits=input.map(line => {
      val splits=line.split("\t")
      splits
    })
    val test=splits.take(5)
    val extractResult:RDD[Info]=splits.map(splits =>
    {
      val  time=splits(0)
      val  url=splits(1)
      val  traffic=splits(2)
      val  ip=splits(3)

//      扩充日期，
      val timeObject=DateUtils.OUTPUT_TIME_FORMAT.parse(time)
      val date=DateUtils.getDate(timeObject)

      var urlType=""
      var urlId=0L
//      正则type,id
      val base="http://www.imooc.com/"
      val baseIndex=url.indexOf(base)
      if(baseIndex>=0)
        {
          val typeIdStr=url.substring(baseIndex+base.length)
          val typeIdSplits=typeIdStr.split("/")
          if(typeIdSplits.length>1){
//            println(typeIdStr,typeIdSplits(0),typeIdSplits(1))
            urlType=typeIdSplits(0)
//            code,video ,learn
            if("video".equals(urlType) || "code".equals(urlType) || "learn".equals(urlType))
              {
                try{
                   urlId=typeIdSplits(1).toLong
                }catch {
                  case e:Exception => {}
                }

              }
            //            article
            else if("article".equals(urlType)){
              val number=RegexUtils.findStartNumber(typeIdSplits(1))
              if(StringUtils.isNotEmpty(number)){
                urlId=number.toLong
              }
            }

          }
        }
//      城市
        val city = IpHelper.findRegionByIp(ip)
      Info(url, urlType, urlId, traffic, ip, city, time, date)
    })
//    extractResult.take(10).foreach(println)
    val infoDF:DataFrame=spark.createDataFrame(extractResult)
    infoDF.printSchema()
    infoDF.show(false)

    infoDF.coalesce(1).write.format("csv").mode(SaveMode.Overwrite).partitionBy("date").save(pathConfig.ExtractOutputPathCSV)
    infoDF.coalesce(1).write.format("parquet").mode(SaveMode.Overwrite).partitionBy("date").save(pathConfig.ExtractOutputPath)
//    split得到4部分
//    解析更多信息
    spark.stop()
  }

}

```

### 指标统计
```
package com.imooc

import com.imooc.conf.pathConfig
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

object TopNJob {
  def main(args: Array[String]): Unit = {
    val sparkconf:SparkConf=new SparkConf().setMaster("local[*]").setAppName("TopNJob")
    val spark:SparkSession=SparkSession.builder().config(sparkconf).getOrCreate()
    val sc:SparkContext=spark.sparkContext

    val inputDF=spark.read.format("parquet").load(pathConfig.ExtractOutputPath)
    inputDF.cache()

    import spark.implicits._

//    topN video
    val videoDF=inputDF.filter( $"date"=== "1970-01-01" &&$"urlType" === "video" && $"urlId" =!= "0").groupBy("date","urlId").agg(count("urlId").as("times")).orderBy($"times".desc)
    val window=Window.partitionBy(col("date")).orderBy(col("times").desc)
    val topNDF=videoDF.withColumn("topn",row_number().over(window)).where(col("topn")<=5)
    topNDF.show(10)

//    topN video in different city
    val videoCityDF=inputDF.filter( $"date"=== "1970-01-01" &&$"urlType" === "video" && $"urlId" =!= "0").groupBy("date","urlId","city").agg(count("urlId").as("times")).orderBy($"times".desc)
    val window1=Window.partitionBy(col("date")).orderBy(col("times").desc)
    val topNCityDF=videoCityDF.withColumn("topn",row_number().over(window1)).where(col("topn")<=5)
    topNCityDF.show(10)
//  code最多的课程
    val codeDF=inputDF.filter( $"date"=== "1970-01-01" &&$"urlType" === "code" && $"urlId" =!= "0").groupBy("date","urlId").agg(count("urlId").as("times")).orderBy($"times".desc)
    val window2=Window.partitionBy(col("date")).orderBy(col("times").desc)
    val topNCodeDF=codeDF.withColumn("topn",row_number().over(window2)).where(col("topn")<=5)
    topNCodeDF.show(10)
//  访问量最多的文章
    val articleDF=inputDF.filter( $"date"=== "1970-01-01" &&$"urlType" === "article" && $"urlId" =!= "0").groupBy("date","urlId").agg(count("urlId").as("times")).orderBy($"times".desc)
    val window3=Window.partitionBy(col("date")).orderBy(col("times").desc)
    val topNArticleDF=articleDF.withColumn("topn",row_number().over(window3)).where(col("topn")<=5)
    topNArticleDF.show(10)
  }
}

```






















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































